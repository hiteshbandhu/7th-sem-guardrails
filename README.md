# hit.rails

AI Guardrails for LLM Applications - Add safety rails to your LLM-powered applications with prompt filtering and output safety checks.

## Features

- **Prompt Filtering**: Prevent harmful or unwanted prompts from reaching your LLM
- **Safe Outputs**: Validate and sanitize LLM responses before they reach your users

## Getting Started

### Prerequisites

- Node.js 18+ 
- npm, yarn, or pnpm

### Installation

1. Clone the repository:
